{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***PRE-PROCESSING***"
      ],
      "metadata": {
        "id": "FCk_bzpbAmky"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xk-duoG9HbH",
        "outputId": "3560365e-659b-4469-f234-a8ecb57408a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "#TOKENIZATION\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "para = \"I am lost form the light, who was born from the shadows. I am the rightful heir of death and bastard son of fate. Wherever I go, ruin follows. If you had any sense, you would have run away as soon as you saw me.\""
      ],
      "metadata": {
        "id": "2TDsNJbn9oMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence tokenization\n",
        "sent_tokens = nltk.sent_tokenize(para)\n",
        "print(\"Sentence Tokenized : \", sent_tokens)\n",
        "\n",
        "#word tokenization\n",
        "word_tokens = nltk.word_tokenize(para)\n",
        "print(\"Word Tokenized : \", word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pN4wm8pu-DCI",
        "outputId": "91f81cad-f87f-4a75-b845-225e9b358521"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenized :  ['I am lost form the light, who was born from the shadows.', 'I am the rightful heir of death and bastard son of fate.', 'Wherever I go, ruin follows.', 'If you had any sense, you would have run away as soon as you saw me.']\n",
            "Word Tokenized :  ['I', 'am', 'lost', 'form', 'the', 'light', ',', 'who', 'was', 'born', 'from', 'the', 'shadows', '.', 'I', 'am', 'the', 'rightful', 'heir', 'of', 'death', 'and', 'bastard', 'son', 'of', 'fate', '.', 'Wherever', 'I', 'go', ',', 'ruin', 'follows', '.', 'If', 'you', 'had', 'any', 'sense', ',', 'you', 'would', 'have', 'run', 'away', 'as', 'soon', 'as', 'you', 'saw', 'me', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Stopwords removal***"
      ],
      "metadata": {
        "id": "hm7G9u-3AjAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stopword removal\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(\"Stop Words : \", stop_words)\n",
        "nstop = []\n",
        "for w in word_tokens:\n",
        "  if w not in stop_words:\n",
        "    nstop.append(w)\n",
        "\n",
        "print(\"After removing stopwords :\", nstop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oypK1H2EAaBn",
        "outputId": "441288cc-0b19-40b1-aa37-4c266d9a01c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop Words :  {'into', 'myself', 'such', 'is', 'we', 'through', 'against', 'over', \"needn't\", 'between', 'an', 'whom', 'had', \"you're\", 'they', 'below', 'just', 'i', 'hers', 'won', 'because', 'themselves', 'or', 'your', 'does', 'so', 'until', 'aren', 'are', 'to', 'himself', 'other', \"you'd\", 'been', 'my', 'on', 'who', 'their', 'doesn', 'only', 'o', 'while', 'd', 'ain', \"haven't\", 'here', 'both', 'weren', 'these', 'off', \"don't\", 'once', 'do', 'them', 'with', 'those', 'no', \"won't\", 'all', 'why', 'shouldn', 'his', 'during', 'for', 've', \"she's\", 'down', 'there', \"hasn't\", 're', \"should've\", \"isn't\", 'isn', 'will', 'was', 'by', \"wouldn't\", \"it's\", 'before', 'm', 'any', 'me', 'he', \"shan't\", 'where', 'in', \"doesn't\", 'yourself', 'have', 'at', 's', \"didn't\", 'when', 'can', 'further', 'being', 'now', 'than', 'it', 'few', 'the', 'and', \"wasn't\", 'own', 'haven', 'after', 'wouldn', 'under', 'each', 'more', 'am', \"you've\", 'if', 'not', 'theirs', 'don', 'same', 'then', \"weren't\", 'some', 'yours', 'itself', 'what', 'were', 'too', 'herself', 'this', 'did', 'out', 'from', 'she', 'him', 'very', 'up', 'wasn', 'hadn', 'mightn', 't', 'most', 'a', 'as', 'didn', \"couldn't\", 'll', 'needn', \"mightn't\", 'y', \"hadn't\", \"you'll\", 'about', 'shan', 'has', 'ours', 'yourselves', 'ourselves', 'but', \"mustn't\", 'again', 'having', 'mustn', 'its', \"shouldn't\", 'of', 'be', 'ma', 'that', 'should', 'above', 'hasn', 'nor', 'doing', 'you', 'her', \"aren't\", 'our', 'how', 'couldn', 'which', \"that'll\"}\n",
            "After removing stopwords : ['I', 'lost', 'form', 'light', ',', 'born', 'shadows', '.', 'I', 'rightful', 'heir', 'death', 'bastard', 'son', 'fate', '.', 'Wherever', 'I', 'go', ',', 'ruin', 'follows', '.', 'If', 'sense', ',', 'would', 'run', 'away', 'soon', 'saw', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Stemming and Lemmatization***"
      ],
      "metadata": {
        "id": "MxI8BC2hGQOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = []\n",
        "for w in nstop:\n",
        "  print(w, \" : \", stemmer.stem(w))\n",
        "  stemmed.append(stemmer.stem(w))\n",
        "\n",
        "print(\"Stemmed : \", stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGlOmBjDA80g",
        "outputId": "c146a1ae-9fe3-43db-bb2e-412190ed2181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I  :  i\n",
            "lost  :  lost\n",
            "form  :  form\n",
            "light  :  light\n",
            ",  :  ,\n",
            "born  :  born\n",
            "shadows  :  shadow\n",
            ".  :  .\n",
            "I  :  i\n",
            "rightful  :  right\n",
            "heir  :  heir\n",
            "death  :  death\n",
            "bastard  :  bastard\n",
            "son  :  son\n",
            "fate  :  fate\n",
            ".  :  .\n",
            "Wherever  :  wherev\n",
            "I  :  i\n",
            "go  :  go\n",
            ",  :  ,\n",
            "ruin  :  ruin\n",
            "follows  :  follow\n",
            ".  :  .\n",
            "If  :  if\n",
            "sense  :  sens\n",
            ",  :  ,\n",
            "would  :  would\n",
            "run  :  run\n",
            "away  :  away\n",
            "soon  :  soon\n",
            "saw  :  saw\n",
            ".  :  .\n",
            "Stemmed :  ['i', 'lost', 'form', 'light', ',', 'born', 'shadow', '.', 'i', 'right', 'heir', 'death', 'bastard', 'son', 'fate', '.', 'wherev', 'i', 'go', ',', 'ruin', 'follow', '.', 'if', 'sens', ',', 'would', 'run', 'away', 'soon', 'saw', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmetization\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmetizer = WordNetLemmatizer()\n",
        "lemm = []\n",
        "\n",
        "for w in stemmed:\n",
        "  print(w, \":\", lemmetizer.lemmatize(w))\n",
        "  lemm.append(lemmetizer.lemmatize(w))\n",
        "\n",
        "print(\"After lemmatization: \", lemm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpRIri6XCnz2",
        "outputId": "989c49d4-20c0-4d30-c254-4e1ba0e47227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i : i\n",
            "lost : lost\n",
            "form : form\n",
            "light : light\n",
            ", : ,\n",
            "born : born\n",
            "shadow : shadow\n",
            ". : .\n",
            "i : i\n",
            "right : right\n",
            "heir : heir\n",
            "death : death\n",
            "bastard : bastard\n",
            "son : son\n",
            "fate : fate\n",
            ". : .\n",
            "wherev : wherev\n",
            "i : i\n",
            "go : go\n",
            ", : ,\n",
            "ruin : ruin\n",
            "follow : follow\n",
            ". : .\n",
            "if : if\n",
            "sens : sen\n",
            ", : ,\n",
            "would : would\n",
            "run : run\n",
            "away : away\n",
            "soon : soon\n",
            "saw : saw\n",
            ". : .\n",
            "After lemmatization:  ['i', 'lost', 'form', 'light', ',', 'born', 'shadow', '.', 'i', 'right', 'heir', 'death', 'bastard', 'son', 'fate', '.', 'wherev', 'i', 'go', ',', 'ruin', 'follow', '.', 'if', 'sen', ',', 'would', 'run', 'away', 'soon', 'saw', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***WORD2VEC***"
      ],
      "metadata": {
        "id": "upsieuV2Ed_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22hSsRON3nlu",
        "outputId": "1008437e-4c70-48d9-a374-f4709a9b5ddb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.4)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_sentences = [\n",
        "    [word for word in nltk.word_tokenize(sentence.lower()) if word not in stop_words]\n",
        "    for sentence in sent_tokens\n",
        "]\n",
        "print(\"Word Tokenized: \", tokenized_sentences)\n",
        "model = Word2Vec(sentences=tokenized_sentences, min_count=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zOikOWRX3is6",
        "outputId": "637572a6-2620-4d4f-ce02-1961c99fea47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenized:  [['lost', 'form', 'light', ',', 'born', 'shadows', '.'], ['rightful', 'heir', 'death', 'bastard', 'son', 'fate', '.'], ['wherever', 'go', ',', 'ruin', 'follows', '.'], ['sense', ',', 'would', 'run', 'away', 'soon', 'saw', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.similar_by_word('death')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzcoiUEB3q1P",
        "outputId": "0432c61b-0f95-4e56-9071-7b9576201c3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('soon', 0.12812119722366333),\n",
              " ('son', 0.10947464406490326),\n",
              " ('shadows', 0.10898242145776749),\n",
              " ('lost', 0.09931963682174683),\n",
              " ('follows', 0.09614686667919159),\n",
              " ('would', 0.0863887295126915),\n",
              " ('.', 0.06285606324672699),\n",
              " ('born', 0.05057644471526146),\n",
              " ('rightful', 0.026872780174016953),\n",
              " ('light', 0.020000817254185677)]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.words_closer_than('shadows', 'light')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Obiu6H5W4nsV",
        "outputId": "fe9626d5-59a3-44bd-c952-caaff3ae95c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-41-748338a058c9>:1: DeprecationWarning: Call to deprecated `words_closer_than` (Use closer_than instead).\n",
            "  model.wv.words_closer_than('shadows', 'light')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.',\n",
              " 'saw',\n",
              " 'bastard',\n",
              " 'form',\n",
              " 'born',\n",
              " 'death',\n",
              " 'son',\n",
              " 'soon',\n",
              " 'fate',\n",
              " 'wherever',\n",
              " 'go',\n",
              " 'ruin',\n",
              " 'follows',\n",
              " 'sense',\n",
              " 'would',\n",
              " 'run',\n",
              " 'lost']"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    }
  ]
}